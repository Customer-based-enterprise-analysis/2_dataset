{"cells":[{"cell_type":"code","execution_count":null,"id":"6b13b096","metadata":{"id":"6b13b096"},"outputs":[],"source":["import warnings\n","warnings.filterwarnings('ignore')\n","\n","from matplotlib import rc\n","import matplotlib.pyplot as plt \n","rc('font',family='Malgun Gothic') # í•œê¸€\n","plt.rcParams['axes.unicode_minus'] = False # ë§ˆì´ë„ˆìŠ¤ ë¶€í˜¸\n","\n","import os\n","import pandas as pd\n","import seaborn as sns\n","import numpy as np\n","from IPython.core.display import HTML\n","import time\n","import re"]},{"cell_type":"markdown","id":"36778590","metadata":{"id":"36778590"},"source":["# <font color=red>__ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸°__</font>"]},{"cell_type":"code","execution_count":null,"id":"4491b551","metadata":{"id":"4491b551"},"outputs":[],"source":["# íŒŒì¼ ë¶ˆëŸ¬ì™€ì„œ ë³€ìˆ˜ì— ì €ì¥\n","path = './dataset'\n","file_list = os.listdir(path)\n","\n","data_li = []\n","for file in file_list:\n","    data = pd.read_csv(f'dataset/{file}',encoding='cp949')\n","    file_name = file.replace('.txt','')\n","    globals()[file_name] = data\n","    data_li.append(file_name)"]},{"cell_type":"code","execution_count":null,"id":"a4bc7804","metadata":{"id":"a4bc7804"},"outputs":[],"source":["# purprd êµ¬ë§¤ì¼ì type ë³€ê²½ ë° ë…„, ë°˜ê¸°, ë¶„ê¸°, ìš”ì¼ ì¶”ê°€\n","purprd['êµ¬ë§¤ì¼ì'] = pd.to_datetime(purprd['êµ¬ë§¤ì¼ì'], format='%Y%m%d')\n","purprd['year'] = purprd['êµ¬ë§¤ì¼ì'].dt.year\n","purprd['quarter'] = purprd['êµ¬ë§¤ì¼ì'].dt.quarter\n","purprd['weekday'] = purprd['êµ¬ë§¤ì¼ì'].dt.weekday # ì›” 0 ~ ì¼ 6\n","\n","def to_half(year,quarter):\n","    if (year==2014) & (quarter in [1,2]):\n","        return 1\n","    elif (year==2014) & (quarter in [3,4]):\n","        return 2\n","    elif (year==2015) & (quarter in [1,2]):\n","        return 3\n","    else:\n","        return 4\n","        \n","purprd['half'] = purprd.apply(lambda x: to_half(x['year'], x['quarter']), axis=1)"]},{"cell_type":"code","execution_count":null,"id":"c6c86228","metadata":{"id":"c6c86228"},"outputs":[],"source":["# ê¸°ì¡´ê³ ê° => ë§¤ ë¶„ê¸° 1íšŒ ì´ìƒ êµ¬ë§¤í•œ ê³ ê°ìœ¼ë¡œ í•œì •\n","# ê¸°ì¡´ê³ ê°ë§Œ ë‚¨ê¸´ dataframe ìƒì„±\n","all_cust = pd.pivot_table(purprd,\n","                         index='ê³ ê°ë²ˆí˜¸',\n","                         columns='half',\n","                         values='êµ¬ë§¤ê¸ˆì•¡',\n","                         aggfunc='sum')\n","\n","existing_cust_idx = all_cust.dropna().index.tolist() # ê¸°ì¡´ê³ ê° ê³ ê°ë²ˆí˜¸\n","\n","for data in data_li:\n","    try:\n","        globals()[data] = globals()[data].query(f'ê³ ê°ë²ˆí˜¸ == {existing_cust_idx}')\n","    except:\n","        pass"]},{"cell_type":"code","execution_count":null,"id":"d64536d7","metadata":{"id":"d64536d7"},"outputs":[],"source":["# ì—°ë ¹ëŒ€ ë¬¶ì–´ì¤Œ\n","def cat_age(age):\n","    if age == '19ì„¸ì´í•˜':\n","        return 10\n","    elif age in ['20ì„¸~24ì„¸', '25ì„¸~29ì„¸']:\n","        return 20\n","    elif age in ['30ì„¸~34ì„¸', '35ì„¸~39ì„¸']:\n","        return 30\n","    elif age in ['40ì„¸~44ì„¸', '45ì„¸~49ì„¸']:\n","        return 40\n","    elif age in ['50ì„¸~54ì„¸', '55ì„¸~59ì„¸']:\n","        return 50\n","    else:\n","        return 60\n","    \n","cust['ì—°ë ¹ëŒ€'] = cust['ì—°ë ¹ëŒ€'].apply(lambda x: cat_age(x))"]},{"cell_type":"markdown","id":"499aa410","metadata":{"id":"499aa410"},"source":["# <font color=red>__ì¢…ì†ë³€ìˆ˜__</font>"]},{"cell_type":"code","execution_count":null,"id":"6992f0e4","metadata":{"id":"6992f0e4"},"outputs":[],"source":["# ì¢…ì†ë³€ìˆ˜\n","def get_label(p1, p2):\n","    \"\"\"\n","    ì „ì²´ ë§¤ì¶œ ì¦ê°ìœ¨ì„ ê³ ë ¤í•œ ê³ ê°ë³„ ë§¤ì¶œ ì¦ê°ìœ¨(ë°˜ê¸° ê¸°ì¤€)\n","    -> ê°ì†Œê³ ê° : 1\n","    -> ì¦ê°€ê³ ê° : 0\n","    \"\"\"\n","    sales = pd.pivot_table(purprd,index='ê³ ê°ë²ˆí˜¸', # ê³ ê°ë³„ ë°˜ê¸° ë§¤ì¶œ\n","                              columns = 'half',\n","                              values = 'êµ¬ë§¤ê¸ˆì•¡',\n","                              aggfunc= 'sum')\n","    rate_variation = (sum(sales[int(p2)])-sum(sales[int(p1)]))/sum(sales[int(p1)]) # ì „ì²´ ë§¤ì¶œ ì¦ê°ìœ¨\n","    sales[f'y'] = (sales[int(p2)] - sales[int(p1)])/sales[int(p1)]/rate_variation # ê³ ê°ë³„ ë§¤ì¶œ ì¦ê°ìœ¨\n","    \n","    def to_label(sales_variation): # ë§¤ì¶œ ê°ì†Œ ê³ ê° : 1\n","        if sales_variation >= 0:\n","            return 0\n","        else:\n","            return 1\n","    \n","    sales[f'y'] = sales[f'y'].apply(lambda x: to_label(x))\n","    sales = sales[[f'y']]\n","    return sales"]},{"cell_type":"markdown","id":"0eadeb13","metadata":{"id":"0eadeb13"},"source":["# <font color=red>__ë…ë¦½ë³€ìˆ˜__</font>"]},{"cell_type":"markdown","id":"f313c1dd","metadata":{"id":"f313c1dd"},"source":["# 1) membership"]},{"cell_type":"code","execution_count":null,"id":"1d856433","metadata":{"id":"1d856433"},"outputs":[],"source":["# membership ê°€ì… ê°œìˆ˜\n","def membership_count():\n","    membership_cust = pd.pivot_table(membership,\n","                                      index='ê³ ê°ë²ˆí˜¸',\n","                                      columns='ë©¤ë²„ì‹­ëª…',\n","                                      values='ê°€ì…ë…„ì›”',\n","                                      aggfunc='count').fillna(0)\n","    membership_cust['ê°€ì…ê°œìˆ˜'] = membership_cust.sum(axis=1)\n","    return membership_cust[['ê°€ì…ê°œìˆ˜']]"]},{"cell_type":"code","execution_count":null,"id":"3c4b7e32","metadata":{"id":"3c4b7e32"},"outputs":[],"source":["# ìµœì´ˆ membership ê°€ì…ë…„ë„\n","def membership_date():\n","    membership['ê°€ì…ë…„ì›”'] = pd.to_datetime(membership['ê°€ì…ë…„ì›”'], format='%Y%m')\n","    membership['ê°€ì…ë…„ë„'] = membership['ê°€ì…ë…„ì›”'].dt.year\n","\n","    first_membership_date = pd.pivot_table(membership,\n","                                    index='ê³ ê°ë²ˆí˜¸',\n","                                    values='ê°€ì…ë…„ë„',\n","                                    aggfunc='min')\n","    return first_membership_date"]},{"cell_type":"markdown","id":"4a6808c3","metadata":{"id":"4a6808c3"},"source":["# 2) channel"]},{"cell_type":"code","execution_count":null,"id":"20da220c","metadata":{"id":"20da220c"},"outputs":[],"source":["# app login íšŸìˆ˜\n","def app_count():\n","    channel_count = pd.pivot_table(channel,\n","                  index='ê³ ê°ë²ˆí˜¸',\n","                  columns='ì œíœ´ì‚¬',\n","                  values='ì´ìš©íšŸìˆ˜')\n","\n","    channel_count['APPë¡œê·¸ì¸íšŸìˆ˜'] = channel_count[channel_count.columns[channel_count.columns.str.contains('APP')]].sum(axis=1)\n","    return channel_count[['APPë¡œê·¸ì¸íšŸìˆ˜']]"]},{"cell_type":"markdown","id":"8dacd130","metadata":{"id":"8dacd130"},"source":["# 3) compuse"]},{"cell_type":"code","execution_count":null,"id":"d64e2ca5","metadata":{"id":"d64e2ca5"},"outputs":[],"source":["# Bì œíœ´ì‚¬ ê²½ìŸì‚¬ ì´ìš©ë¥ \n","def B_compuse_rate():\n","    compuse_count = pd.pivot_table(compuse,\n","                    index='ê³ ê°ë²ˆí˜¸',\n","                    columns='ê²½ìŸì‚¬',\n","                    values='ì œíœ´ì‚¬',\n","                    aggfunc='count').fillna(0)\n","    compuse_count['c_B'] = compuse_count['B01'] + compuse_count['B02']\n","\n","    purprd_count = pd.pivot_table(purprd.drop_duplicates(subset='ì˜ìˆ˜ì¦ë²ˆí˜¸'),\n","                                  index='ê³ ê°ë²ˆí˜¸',\n","                                  columns='ì œíœ´ì‚¬',\n","                                  values='ì˜ìˆ˜ì¦ë²ˆí˜¸',\n","                                  aggfunc='count').fillna(0)\n","\n","    compuse_count = compuse_count.join(purprd_count)\n","\n","    compuse_count[f'c_B_rate'] = round(compuse_count['c_B']/(compuse_count['c_B']+compuse_count['B'])*100, 2)\n","    return compuse_count[['c_B_rate']]"]},{"cell_type":"markdown","id":"c1771a38","metadata":{"id":"c1771a38"},"source":["#### prodcat"]},{"cell_type":"code","execution_count":null,"id":"55755be6","metadata":{"id":"55755be6"},"outputs":[],"source":["# ìƒí’ˆë¶„ë¥˜ - ëŒ€ë¶„ë¥˜, êµ¬ë§¤ëª©ì ë¶„ë¥˜ ì¶”ê°€(ìˆ˜ì‘ì—…)\n","cat_name = pd.read_excel('ìƒí’ˆë¶„ë¥˜.xlsx', index_col=0)[['ì†Œë¶„ë¥˜ì½”ë“œ','ëŒ€ë¶„ë¥˜','êµ¬ë§¤ëª©ì ë¶„ë¥˜']]\n","prodcat = pd.merge(prodcat, cat_name, on=['ì†Œë¶„ë¥˜ì½”ë“œ'])\n","purprd = pd.merge(purprd, prodcat[['ì†Œë¶„ë¥˜ì½”ë“œ','ì¤‘ë¶„ë¥˜ëª…','ì†Œë¶„ë¥˜ëª…','ëŒ€ë¶„ë¥˜','êµ¬ë§¤ëª©ì ë¶„ë¥˜']])"]},{"cell_type":"markdown","id":"c4977047","metadata":{"id":"c4977047"},"source":["# 5) purprd"]},{"cell_type":"code","execution_count":null,"id":"711be4c6","metadata":{"id":"711be4c6"},"outputs":[],"source":["### ë¹„ìœ¨ ê³„ì‚°\n","def to_rate(df, name):\n","    total = df.sum(axis=1)\n","    for col in df.columns:\n","        df[f'{col}_{name}_rate'] = round(df[col]/total*100, 2)\n","        df.drop(col, axis=1, inplace=True)\n","    return df"]},{"cell_type":"code","execution_count":null,"id":"0f674e44","metadata":{"id":"0f674e44"},"outputs":[],"source":["# ì„ ë§¤í’ˆ, í¸ì˜í’ˆ êµ¬ë§¤ê¸ˆì•¡ ë¹„ì¤‘\n","def purpose_cat_amount(p1, p2):\n","    purpose_cat = pd.pivot_table(purprd.query(f'half==[{p1},{p2}]'),\n","                                      index='ê³ ê°ë²ˆí˜¸',\n","                                      columns='êµ¬ë§¤ëª©ì ë¶„ë¥˜',\n","                                      values='êµ¬ë§¤ê¸ˆì•¡',\n","                                      aggfunc='sum').fillna(0)\n","    to_rate(purpose_cat, 'amount')\n","\n","    purpose_cat.drop('ì „ë¬¸í’ˆ_amount_rate',axis=1,inplace=True)\n","    return purpose_cat"]},{"cell_type":"code","execution_count":null,"id":"cad34009","metadata":{"id":"cad34009"},"outputs":[],"source":["# ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤íšŸìˆ˜ ë¹„ì¤‘\n","def major_cat_count(p1, p2):\n","    major_cat_count = pd.pivot_table(purprd.query(f'half==[{p1},{p2}]'),\n","                                      index='ê³ ê°ë²ˆí˜¸',\n","                                      columns='ëŒ€ë¶„ë¥˜',\n","                                      values='êµ¬ë§¤ê¸ˆì•¡',\n","                                      aggfunc='count').fillna(0)\n","\n","    to_rate(major_cat_count, 'count')\n","    return major_cat_count[['ë¯¸ìš©í’ˆ_count_rate','ìŠ¤í¬ì¸ ë ˆì €_count_rate','íŒ¨ì…˜ì¡í™”_count_rate',\\\n","                    'ì˜ë¥˜_count_rate','ì¸í…Œë¦¬ì–´_count_rate']]"]},{"cell_type":"code","execution_count":null,"id":"694637ff","metadata":{"id":"694637ff"},"outputs":[],"source":["# ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤ê¸ˆì•¡ ë¹„ì¤‘\n","def major_cat_amount(p1, p2):\n","    major_cat_amount = pd.pivot_table(purprd.query(f'half==[{p1},{p2}]'),\n","                                      index='ê³ ê°ë²ˆí˜¸',\n","                                      columns='ëŒ€ë¶„ë¥˜',\n","                                      values='êµ¬ë§¤ê¸ˆì•¡',\n","                                      aggfunc='sum').fillna(0)\n","\n","    to_rate(major_cat_amount, 'amount')\n","    return major_cat_amount[['ê°€ê³µì‹í’ˆ_amount_rate','êµìœ¡ë¬¸í™”_amount_rate','ê¸°íƒ€_amount_rate',\\\n","                     'ì‹ ì„ ì‹í’ˆ_amount_rate','ì¼ìƒìš©í’ˆ_amount_rate']]"]},{"cell_type":"code","execution_count":null,"id":"61fce654","metadata":{"id":"61fce654"},"outputs":[],"source":["# ì œíœ´ì‚¬ë³„ êµ¬ë§¤íšŸìˆ˜ ë¹„ì¤‘\n","def affiliate_count(p1, p2):\n","    affiliate_count = pd.pivot_table(purprd.query(f'half==[{p1},{p2}]'),\n","                                      index='ê³ ê°ë²ˆí˜¸',\n","                                      columns='ì œíœ´ì‚¬',\n","                                      values='êµ¬ë§¤ê¸ˆì•¡',\n","                                      aggfunc='count').fillna(0)\n","    to_rate(affiliate_count, 'count')\n","    return affiliate_count"]},{"cell_type":"code","execution_count":null,"id":"abbaccc1","metadata":{"id":"abbaccc1"},"outputs":[],"source":["# ì œíœ´ì‚¬ë³„ êµ¬ë§¤ê¸ˆì•¡ ë¹„ì¤‘\n","def affiliate_mount(p1, p2):\n","    affiliate_mount = pd.pivot_table(purprd.query(f'half==[{p1},{p2}]'),\n","                                      index='ê³ ê°ë²ˆí˜¸',\n","                                      columns='ì œíœ´ì‚¬',\n","                                      values='êµ¬ë§¤ê¸ˆì•¡',\n","                                      aggfunc='sum').fillna(0)\n","    to_rate(affiliate_mount, 'mount')\n","    return affiliate_mount"]},{"cell_type":"code","execution_count":null,"id":"38d4b5fd","metadata":{"id":"38d4b5fd"},"outputs":[],"source":["### ì¦ê°ìœ¨ ê³„ì‚°(purprd, êµ¬ë§¤ê¸ˆì•¡ ê¸°ì¤€)\n","def purprd_amount_pv(col, period1, period2):\n","    for i in [period1, period2]:\n","        globals()[f'p{i}'] = pd.pivot_table(purprd.query(f'half=={i}'),\n","                                           index='ê³ ê°ë²ˆí˜¸',\n","                                           columns=col,\n","                                           values='êµ¬ë§¤ê¸ˆì•¡',\n","                                           aggfunc='sum').fillna(0)\n","        \n","    variation = (globals()[f'p{period2}'] - globals()[f'p{period1}'])/globals()[f'p{period1}']*100\n","    return variation.replace({np.inf:100, np.nan:0})"]},{"cell_type":"markdown","id":"0aee6360","metadata":{"id":"0aee6360"},"source":["# <font color=red>__dataset__</font>"]},{"cell_type":"code","execution_count":null,"id":"5e595de0","metadata":{"id":"5e595de0"},"outputs":[],"source":["def make_dataset(p1, p2, p3):\n","    dataset = pd.DataFrame(cust[['ì—°ë ¹ëŒ€','ì„±ë³„']]).join([ # ì—°ë ¹ëŒ€, ì„±ë³„    -> - / label\n","        membership_count(), # membership ê°€ì… ê°œìˆ˜                           \n","        membership_date(), # ìµœì´ˆ membership ê°€ì…ë…„ë„                        -> label\n","        app_count(), # app login íšŸìˆ˜                                        \n","        B_compuse_rate(), # Bì œíœ´ì‚¬ ê²½ìŸì‚¬ ì´ìš©ë¥                             \n","        purpose_cat_amount(p1, p2), # ì„ ë§¤í’ˆ, í¸ì˜í’ˆ êµ¬ë§¤ê¸ˆì•¡ ë¹„ì¤‘           \n","        major_cat_count(p1, p2), # ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤íšŸìˆ˜ ë¹„ì¤‘(ë¯¸ìš©í’ˆ,ìŠ¤í¬ì¸ ë ˆì €,íŒ¨ì…˜ì¡í™”,ì˜ë¥˜,ì¸í…Œë¦¬ì–´) \n","        major_cat_amount(p1, p2), # ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤ê¸ˆì•¡ ë¹„ì¤‘(ê°€ê³µì‹í’ˆ,êµìœ¡ë¬¸í™”,ê¸°íƒ€,ì‹ ì„ ì‹í’ˆ,ì¼ìƒìš©í’ˆ)\n","        affiliate_count(p1, p2), # ì œíœ´ì‚¬ë³„ êµ¬ë§¤íšŸìˆ˜ ë¹„ì¤‘\n","        affiliate_mount(p1, p2), # ì œíœ´ì‚¬ë³„ êµ¬ë§¤ê¸ˆì•¡ ë¹„ì¤‘\n","        purprd_amount_pv('êµ¬ë§¤ëª©ì ë¶„ë¥˜', p1, p2)[['í¸ì˜í’ˆ']], # í¸ì˜í’ˆ êµ¬ë§¤ê¸ˆì•¡ ì¦ê°ìœ¨\n","        purprd_amount_pv('ëŒ€ë¶„ë¥˜', p1, p2)[['ê°€ê³µì‹í’ˆ','ë¯¸ìš©í’ˆ','ìŠ¤í¬ì¸ ë ˆì €','ì˜ë¥˜']], # ëŒ€ë¶„ë¥˜ë³„ êµ¬ë§¤ê¸ˆì•¡ ì¦ê°ìœ¨\n","        purprd_amount_pv('ì œíœ´ì‚¬', p1, p2)[['A']], # ì œíœ´ì‚¬ë³„ êµ¬ë§¤ê¸ˆì•¡ ì¦ê°ìœ¨\n","        get_label(p1, p3)]) # ì¢…ì†ë³€ìˆ˜\n","    dataset.fillna(0, inplace=True)\n","    return dataset\n","\n","dataset1 = make_dataset(1,2,3) # train(train / vaild)\n","dataset2 = make_dataset(2,3,4) # test"]},{"cell_type":"code","execution_count":null,"id":"34c58968","metadata":{"id":"34c58968"},"outputs":[],"source":["# Labelencoder\n","from sklearn.preprocessing import LabelEncoder\n","\n","le = LabelEncoder()\n","le_cols = ['ì„±ë³„','ê°€ì…ë…„ë„']\n","\n","for col in le_cols:\n","    dataset1[col] = le.fit_transform(dataset1[col])\n","    dataset2[col] = le.fit_transform(dataset2[col])"]},{"cell_type":"code","execution_count":null,"id":"f4f5ad0b","metadata":{"id":"f4f5ad0b"},"outputs":[],"source":["# Category\n","# cat_cols = dataset1.columns[4:-7].tolist()\n","\n","# def to_cat(df, col, n=6):\n","#     data = df[col].astype(float)\n","#     cat_data = pd.cut(data, n, labels=list(range(1, n+1)))\n","#     return cat_data\n","\n","# for col in cat_cols:\n","#     dataset1[col] = to_cat(dataset1, col)\n","#     dataset2[col] = to_cat(dataset2, col)"]},{"cell_type":"code","execution_count":null,"id":"8c50b8e7","metadata":{"id":"8c50b8e7"},"outputs":[],"source":["# # Category(ì¦ê°ìœ¨ => ë§ˆì´ë„ˆìŠ¤ / í”ŒëŸ¬ìŠ¤ ë‚˜ëˆ„ê³  ì¹´í…Œê³ ë¦¬)\n","# cat_plus_cols = ['í¸ì˜í’ˆ', 'ê°€ê³µì‹í’ˆ', 'ë¯¸ìš©í’ˆ', 'ìŠ¤í¬ì¸ ë ˆì €', 'ì˜ë¥˜', 'A']\n","\n","# def to_cat_plus(df, col, n1=3, n2=3):\n","#     data = df[col]\n","    \n","#     data_minus = data[data<=0]\n","#     data_minus_cut = pd.cut(data_minus,n1,labels=list(range(1,n1+1)))\n","\n","#     data_plus = data[0<data]\n","#     data_plus_cut = pd.cut(data_plus,n2,labels=list(range(n1+1,n1+n2+1)))\n","    \n","#     return pd.concat([data_minus_cut,data_plus_cut])\n","\n","# for col in cat_plus_cols:\n","#     dataset1[col] = to_cat_plus(dataset1, col)\n","#     dataset2[col] = to_cat_plus(dataset2, col)"]},{"cell_type":"code","execution_count":null,"id":"3e347ae5","metadata":{"id":"3e347ae5"},"outputs":[],"source":["# Standardscaler\n","# from sklearn.preprocessing import MinMaxScaler\n","\n","# sc = MinMaxScaler()\n","\n","# for col in sc_cols:\n","#     dataset1[col] = sc.fit_transform(dataset1[[col]])\n","#     dataset2[col] = sc.fit_transform(dataset2[[col]])"]},{"cell_type":"code","execution_count":null,"id":"c8b5f4f5","metadata":{"id":"c8b5f4f5"},"outputs":[],"source":["for col in dataset1.dtypes[dataset1.dtypes=='category'].index:\n","    dataset1[col] = dataset1[col].astype(int)\n","    dataset2[col] = dataset2[col].astype(int)"]},{"cell_type":"code","execution_count":null,"id":"dcf2c67f","metadata":{"id":"dcf2c67f"},"outputs":[],"source":["dataset1.to_csv('dataset1.csv')\n","dataset2.to_csv('dataset2.csv')\n","\n","dataset1 = pd.read_csv('dataset1.csv', index_col=0)\n","dataset2 = pd.read_csv('dataset2.csv', index_col=0)"]},{"cell_type":"code","execution_count":null,"id":"fb29414d","metadata":{"id":"fb29414d"},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","X = dataset1.drop('y', axis=1)\n","y = dataset1['y']\n","\n","X_train, X_test, y_train, y_test = train_test_split(X, y,\n","                                                   test_size=0.3, # vaild\n","                                                   random_state=1004)"]},{"cell_type":"code","execution_count":null,"id":"631a684c","metadata":{"id":"631a684c"},"outputs":[],"source":["# #ìµœì¢… TESTì‹œ ì‚¬ìš©\n","# X_train = dataset1.drop('y', axis=1)\n","# y_train = dataset1['y']\n","\n","# X_test = dataset2.drop('y', axis=1)\n","# y_test = dataset2['y']"]},{"cell_type":"code","execution_count":null,"id":"7e45b19d","metadata":{"scrolled":false,"id":"7e45b19d","outputId":"71010ee4-f6ae-4078-fd6c-723331c23790"},"outputs":[{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>rank_test_score</th>\n","      <th>mean_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'max_depth': 3}</td>\n","      <td>2</td>\n","      <td>0.692658</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'max_depth': 5}</td>\n","      <td>1</td>\n","      <td>0.694299</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["DecisionTreeClassifier ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°: {'max_depth': 5}\n","DecisionTreeClassifier ìµœê³  ì •í™•ë„:0.6943\n","í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ì •í™•ë„:0.7010\n"]},{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>rank_test_score</th>\n","      <th>mean_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'max_depth': 7, 'n_estimators': 100}</td>\n","      <td>6</td>\n","      <td>0.707432</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'max_depth': 7, 'n_estimators': 200}</td>\n","      <td>5</td>\n","      <td>0.707656</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'max_depth': 7, 'n_estimators': 300}</td>\n","      <td>9</td>\n","      <td>0.706686</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'max_depth': 9, 'n_estimators': 100}</td>\n","      <td>1</td>\n","      <td>0.711088</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'max_depth': 9, 'n_estimators': 200}</td>\n","      <td>2</td>\n","      <td>0.710939</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>{'max_depth': 9, 'n_estimators': 300}</td>\n","      <td>3</td>\n","      <td>0.709670</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>{'max_depth': 11, 'n_estimators': 100}</td>\n","      <td>7</td>\n","      <td>0.707208</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>{'max_depth': 11, 'n_estimators': 200}</td>\n","      <td>8</td>\n","      <td>0.707208</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>{'max_depth': 11, 'n_estimators': 300}</td>\n","      <td>4</td>\n","      <td>0.708999</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["RandomForestClassifier ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°: {'max_depth': 9, 'n_estimators': 100}\n","RandomForestClassifier ìµœê³  ì •í™•ë„:0.7111\n","í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ì •í™•ë„:0.7133\n"]},{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>rank_test_score</th>\n","      <th>mean_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'C': 1, 'penalty': 'l1'}</td>\n","      <td>9</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'C': 1, 'penalty': 'l2'}</td>\n","      <td>3</td>\n","      <td>0.654081</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'C': 1, 'penalty': 'elasticnet'}</td>\n","      <td>10</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'C': 1, 'penalty': 'none'}</td>\n","      <td>4</td>\n","      <td>0.654007</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'C': 10, 'penalty': 'l1'}</td>\n","      <td>11</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>{'C': 10, 'penalty': 'l2'}</td>\n","      <td>2</td>\n","      <td>0.654231</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>{'C': 10, 'penalty': 'elasticnet'}</td>\n","      <td>12</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>{'C': 10, 'penalty': 'none'}</td>\n","      <td>4</td>\n","      <td>0.654007</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>{'C': 100, 'penalty': 'l1'}</td>\n","      <td>13</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>{'C': 100, 'penalty': 'l2'}</td>\n","      <td>8</td>\n","      <td>0.653559</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>{'C': 100, 'penalty': 'elasticnet'}</td>\n","      <td>14</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>{'C': 100, 'penalty': 'none'}</td>\n","      <td>4</td>\n","      <td>0.654007</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>{'C': 1000, 'penalty': 'l1'}</td>\n","      <td>15</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>{'C': 1000, 'penalty': 'l2'}</td>\n","      <td>1</td>\n","      <td>0.654529</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>{'C': 1000, 'penalty': 'elasticnet'}</td>\n","      <td>16</td>\n","      <td>NaN</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>{'C': 1000, 'penalty': 'none'}</td>\n","      <td>4</td>\n","      <td>0.654007</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["LogisticRegression ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°: {'C': 1000, 'penalty': 'l2'}\n","LogisticRegression ìµœê³  ì •í™•ë„:0.6545\n","í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ì •í™•ë„:0.6590\n","[12:34:47] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:34:49] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:34:51] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:34:53] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:34:56] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:34:59] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:02] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:05] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:09] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:16] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:19] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:30] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:33] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:38] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:43] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n","[12:35:48] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"]},{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>rank_test_score</th>\n","      <th>mean_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300}</td>\n","      <td>1</td>\n","      <td>0.707507</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300}</td>\n","      <td>2</td>\n","      <td>0.702209</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 300}</td>\n","      <td>4</td>\n","      <td>0.697732</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300}</td>\n","      <td>3</td>\n","      <td>0.702209</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}</td>\n","      <td>5</td>\n","      <td>0.696314</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300}</td>\n","      <td>6</td>\n","      <td>0.693404</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["XGBClassifier ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300}\n","XGBClassifier ìµœê³  ì •í™•ë„:0.7075\n","í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ì •í™•ë„:0.7131\n"]},{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>rank_test_score</th>\n","      <th>mean_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300}</td>\n","      <td>1</td>\n","      <td>0.706014</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'learning_rate': 0.05, 'max_depth': 5, 'n_estimators': 300}</td>\n","      <td>3</td>\n","      <td>0.701314</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'learning_rate': 0.05, 'max_depth': 6, 'n_estimators': 300}</td>\n","      <td>2</td>\n","      <td>0.701761</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'learning_rate': 0.1, 'max_depth': 4, 'n_estimators': 300}</td>\n","      <td>4</td>\n","      <td>0.700568</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'learning_rate': 0.1, 'max_depth': 5, 'n_estimators': 300}</td>\n","      <td>6</td>\n","      <td>0.694822</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>{'learning_rate': 0.1, 'max_depth': 6, 'n_estimators': 300}</td>\n","      <td>5</td>\n","      <td>0.696911</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["LGBMClassifier ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°: {'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 300}\n","LGBMClassifier ìµœê³  ì •í™•ë„:0.7060\n","í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ì •í™•ë„:0.7138\n"]},{"data":{"text/html":["<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>params</th>\n","      <th>rank_test_score</th>\n","      <th>mean_test_score</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>{'learning_rate': 0.05, 'n_estimators': 100}</td>\n","      <td>1</td>\n","      <td>0.708477</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>{'learning_rate': 0.05, 'n_estimators': 200}</td>\n","      <td>2</td>\n","      <td>0.708253</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>{'learning_rate': 0.05, 'n_estimators': 300}</td>\n","      <td>3</td>\n","      <td>0.708104</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>{'learning_rate': 0.05, 'n_estimators': 400}</td>\n","      <td>5</td>\n","      <td>0.706462</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>{'learning_rate': 0.05, 'n_estimators': 500}</td>\n","      <td>6</td>\n","      <td>0.705790</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>{'learning_rate': 0.1, 'n_estimators': 100}</td>\n","      <td>4</td>\n","      <td>0.708029</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>{'learning_rate': 0.1, 'n_estimators': 200}</td>\n","      <td>7</td>\n","      <td>0.704447</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>{'learning_rate': 0.1, 'n_estimators': 300}</td>\n","      <td>8</td>\n","      <td>0.703179</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>{'learning_rate': 0.1, 'n_estimators': 400}</td>\n","      <td>9</td>\n","      <td>0.700866</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>{'learning_rate': 0.1, 'n_estimators': 500}</td>\n","      <td>10</td>\n","      <td>0.699896</td>\n","    </tr>\n","  </tbody>\n","</table>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["GradientBoostingClassifier ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°: {'learning_rate': 0.05, 'n_estimators': 100}\n","GradientBoostingClassifier ìµœê³  ì •í™•ë„:0.7085\n","í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ì •í™•ë„:0.7117\n"]}],"source":["from sklearn.tree import DecisionTreeClassifier\n","from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n","from sklearn.linear_model import LogisticRegression\n","from xgboost import XGBClassifier, plot_importance\n","from lightgbm import LGBMClassifier, plot_importance\n","# from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import accuracy_score\n","\n","\n","# ê°ì²´ ìƒì„±\n","dct_clf = DecisionTreeClassifier(criterion = 'entropy')\n","rf_clf = RandomForestClassifier()\n","lr_clf = LogisticRegression()\n","xgb_clf = XGBClassifier()\n","lgb_clf = LGBMClassifier()\n","gb_clf = GradientBoostingClassifier()\n","# svm_clf = SVC(probability=True)\n","\n","# íŒŒë¼ë¯¸í„° ì„¤ì •\n","dct_parameters = {'max_depth':[3,5], }\n","rf_parameters = {'n_estimators':[100,200,300], 'max_depth':[7,9,11]}\n","lr_parameters = { \"penalty\":['l1', 'l2', 'elasticnet', 'none'], 'C': [ 1, 10, 100, 1000]}\n","xgb_parameters = {'n_estimators':[300], 'learning_rate':[0.05, 0.1], 'max_depth':[4,5,6]}\n","lgb_parameters = {'n_estimators':[300], 'learning_rate':[0.05, 0.1], 'max_depth':[4,5,6]}\n","gb_parameters = {'n_estimators':[100,200,300,400,500],'learning_rate':[0.05,0.1]}\n","# svm_parameters = {'kernel':['linear', 'rbf'], 'C':[2,4,6,8,10]}\n","\n","clf_param = [(dct_clf,dct_parameters),(rf_clf,rf_parameters),(lr_clf,lr_parameters),\n","             (xgb_clf,xgb_parameters),(lgb_clf,lgb_parameters),(gb_clf,gb_parameters),\n","             ] # , (svm_clf,svm_parameters)\n","\n","for clf, parameter in clf_param:\n","    grid_clf = GridSearchCV(clf, param_grid=parameter, scoring='accuracy', cv=3, refit=True)\n","    grid_clf.fit(X_train, y_train)\n","    \n","    # êµì°¨ê²€ì¦ ê²°ê³¼ ì¶œë ¥\n","    class_name = clf.__class__.__name__\n","    scores_df = pd.DataFrame(grid_clf.cv_results_)\n","    display(HTML(scores_df[['params','rank_test_score','mean_test_score']].to_html()))\n","    print(f'{class_name} ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„°:', grid_clf.best_params_)\n","    print('{0} ìµœê³  ì •í™•ë„:{1:.4f}'.format(class_name,grid_clf.best_score_))\n","    \n","    # x_testì— ìµœì  í•˜ì´í¼ íŒŒë¼ë¯¸í„° ì ìš©í•˜ì—¬ ë¶„ì„í•œ ê²°ê³¼\n","    best_clf = grid_clf.best_estimator_\n","    pred = best_clf.predict(X_test)\n","    pred_proba = best_clf.predict_proba(X_test)[:,1]\n","    print('í…ŒìŠ¤íŠ¸ ë°ì´í„° ì„¸íŠ¸ ì •í™•ë„:{:.4f}'.format(accuracy_score(y_test,pred)))"]},{"cell_type":"markdown","id":"4525318d","metadata":{"id":"4525318d"},"source":["# <font color=red>__ì•„ë…¸ë°”ë¶„ì„ğŸ¤–__</font>"]},{"cell_type":"code","execution_count":1,"id":"d0e25fc6","metadata":{"id":"d0e25fc6","executionInfo":{"status":"ok","timestamp":1648794449506,"user_tz":-540,"elapsed":6,"user":{"displayName":"ì¡°í˜„ì •","userId":"02569453026248958222"}}},"outputs":[],"source":["# # anova pvalue í•¨ìˆ˜ğŸ‘»\n","from scipy import stats\n","\n","def anova_test(dataset):\n","    num = 1\n","    data = dataset.drop('y',axis=1)\n","    target = dataset.y\n","    data = data.join(target)\n","    \n","    for n in range(len(data.columns[:-1])):\n","        grps = [data[data.columns[-1]].tolist() for _, data in data.groupby(data.columns[n])]        \n","        F, p = stats.f_oneway(*grps)\n","        if p >= 0.05:\n","            print(num, data.columns[n],':', round(p,3),'ë¬´ì˜ë¯¸')\n","        elif p < 0.05:\n","            print(num, data.columns[n],':', round(p,3),'ğŸ˜Š')\n","        num += 1\n","            \n","anova_test(dataset1)"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.3"},"colab":{"name":"2.dataset_1.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":5}